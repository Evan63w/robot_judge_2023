{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW05: Classification and Deep Learning Essentials (due 31st October)\n",
    "\n",
    "**As part of NEXT WEEK'S homework you will have to provide feedback to two of your classmates' essays on Moodle (by Friday Nov 3rd, at 23:59).** On Moodle, you will be automatically assigned to the two essays you have to provide feedback to on Monday Oct 30th at midnight, in case you want to start ahead.\n",
    "\n",
    "In this homework, we focus again on a prediction task (as we did in week 3). Before diving into the coding part of the homework, I would like you to reflect on the following problem and how you would approach it.\n",
    "\n",
    "*Suppose you are a policy advisor to a developing country government that would like to design a social security program to aid individuals with wages lower than \\$1000 per month. However, data collection in this country is very hard due to the lack of technology infrastructures, incentives to misreport income, and geographical barriers. Therefore, you have access to some demographic and employment data at the individual level from all the municipalities in addition to geographical and municipality level features (here, you can be creative about which variables you have access to). However, you have access to income data only for a random set of municipalities.*\n",
    "\n",
    "*How would you decide how to allocate the transfer using the methods you learned for this course? Be very specific on the method and the main variables you would use.*\n",
    "\n",
    "**There is no right or wrong answer here. This is just a conceptual exercise to make you think about the methods we are learning about in real-life problems.** You don't need to write a lot about this; 100 words or even a scheme about your solution to this task would be enough.\n",
    "\n",
    "For this proble, we basically need to predict whether people are making under or over $1,000 per month, based on their available characteristics. Or alternatively, we could interpret this problem as figuring out the cost of this program by estimating the total number of the population who is under the wage threshold. Because the available data is some demographic and employment data from a random set of municipalities, we can use this data to predict population income by municipality. By splitting up the available data into a training and test set, we can create an OLS. The depedent variable would be the amount of people under the $1000 limit, and the independent variables would include total population, average employment types (create a scale where low-paying jobs are 1, high paying jobs are 5, etc), and geographic data (proximity to nearest large city). After an appropriate model has been developed, simply use the available data for other municipalities to estimate the cost of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise\n",
    "\n",
    "Another area of research that is increasingly employing machine learning is that of medical research (a great example of it is [Mullainathan and Obermeyer, 2021](https://www.nber.org/papers/w26168)). The correct prediction of who may encounter a critical clinical condition is fundamental for the allocation of treatments. Indeed, both treatment availability and doctors' time are not infinite. Therefore, correctly predicting who may be more likely to experience a heart attack or develop cancer is extremely important to help these people and not waste precious resources at the same time.\n",
    "\n",
    "In the following, the main goal will be to predict the probability of a heart attack using some health indicators described below. **Note that these are fake data created following the pattern from a dataset with real health indicators.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/HW05.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    int64  \n",
      " 1   sex       303 non-null    int64  \n",
      " 2   cp        303 non-null    int64  \n",
      " 3   trestbps  303 non-null    int64  \n",
      " 4   chol      303 non-null    int64  \n",
      " 5   fbs       303 non-null    int64  \n",
      " 6   restecg   303 non-null    int64  \n",
      " 7   thalach   303 non-null    int64  \n",
      " 8   exang     303 non-null    int64  \n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    int64  \n",
      " 11  ca        303 non-null    int64  \n",
      " 12  thal      303 non-null    int64  \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 33.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attribute Information**\n",
    "\n",
    "- age\n",
    "- sex: 0 = female; 1 = male\n",
    "- cp: chest pain type (4 values)\n",
    "- trestbps: resting blood pressure\n",
    "- chol: serum cholestoral in mg/dl\n",
    "- fbs: fasting blood sugar > 120 mg/dl\n",
    "- restecg: resting electrocardiographic results (values 0,1,2)\n",
    "- thalach: maximum heart rate achieved\n",
    "- exang: exercise induced angina\n",
    "- oldpeak: ST depression induced by exercise relative to rest\n",
    "- slope: the slope of the peak exercise ST segment\n",
    "- ca: number of major vessels (0-3) colored by flourosopy\n",
    "- thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n",
    "- target: 0= less chance of heart attack 1= more chance of heart attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    165\n",
      "0    138\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "print(df['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost\n",
    "\n",
    "In this part you will build a classifier for the likelihood of having an heart attack using xgboost. You have to train, validate your classifier and print the most meaningful metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.61397\n",
      "[1]\tvalidation_0-logloss:0.57519\n",
      "[2]\tvalidation_0-logloss:0.56951\n",
      "[3]\tvalidation_0-logloss:0.52892\n",
      "[4]\tvalidation_0-logloss:0.52756\n",
      "[5]\tvalidation_0-logloss:0.49205\n",
      "[6]\tvalidation_0-logloss:0.46552\n",
      "[7]\tvalidation_0-logloss:0.48584\n",
      "[8]\tvalidation_0-logloss:0.46345\n",
      "[9]\tvalidation_0-logloss:0.48539\n",
      "[10]\tvalidation_0-logloss:0.47788\n",
      "[11]\tvalidation_0-logloss:0.46275\n",
      "[12]\tvalidation_0-logloss:0.45129\n",
      "[13]\tvalidation_0-logloss:0.46861\n",
      "[14]\tvalidation_0-logloss:0.46457\n",
      "[15]\tvalidation_0-logloss:0.48282\n",
      "[16]\tvalidation_0-logloss:0.47281\n",
      "[17]\tvalidation_0-logloss:0.46428\n",
      "[18]\tvalidation_0-logloss:0.46974\n",
      "[19]\tvalidation_0-logloss:0.47036\n",
      "[20]\tvalidation_0-logloss:0.47840\n",
      "[21]\tvalidation_0-logloss:0.49547\n",
      "[22]\tvalidation_0-logloss:0.50798\n",
      "[23]\tvalidation_0-logloss:0.50621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xgboost/sklearn.py:885: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "##TODO train a classifier using early stopping and the logloss evaluation metric \n",
    "\n",
    "xgb_class = XGBClassifier(eval_metric='logloss')\n",
    "xgb_class.fit(X_train, y_train, early_stopping_rounds=12, eval_set=[(X_valid, y_valid)], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  6]\n",
      " [ 1 16]]\n",
      "accuracy: 0.7741935483870968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = xgb_class.predict(X_test)\n",
    "\n",
    "metrics = confusion_matrix(y_test,y_pred)\n",
    "print(metrics)\n",
    "print(\"accuracy:\",accuracy_score(y_test, y_pred))\n",
    "##TODO plot the confusion metrics and calculate the accuracy score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What can you say about the performance of your classifier based on these metrics?**\n",
    "\n",
    "The performance of the classifier is ok. The .77 accuracy score means that 77% of the predictions are correct. The 4 false positives and 3 false negatives mean that some patients are not given the correct diagnosis. The model has a precision of (9/(9+4)) = .69, and recall of (9/(9+3)) = .75, so 25% of people with high risk are missed, which isn't great. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which metrics are better suited to evaluate this model? Calculate and visualize these metrics, and comment on the performance of the model.**\n",
    "\n",
    "Because this is a medical model, the most important metric is the recall, because patients who are 'missed' are then at a high risk and don't know it. The f1 score is also important because the precision and recall are both important in any model. The balanced accuracy score is not important because as shown above, there are roughly equal 1 and 0 targets. auc and roc_auc score are generally important in models to show how effective they are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score: 0.7272727272727273\n",
      "Recall score: 0.9411764705882353\n",
      "f1: 0.8205128205128205\n",
      "Balanced accuracy score: 0.7563025210084033\n",
      "roc_auc_score 0.7563025210084032\n",
      "auc 0.7563025210084032\n",
      "prec, rec: [0.5483871  0.72727273 1.        ] [1.         0.94117647 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (precision_score, recall_score, f1_score, precision_recall_curve,\n",
    "                             balanced_accuracy_score, roc_curve, auc, roc_auc_score)\n",
    "\n",
    "print(\"Precision score:\",precision_score(y_test,y_pred))\n",
    "print(\"Recall score:\",recall_score(y_test,y_pred))\n",
    "print(\"f1:\",f1_score(y_test,y_pred))\n",
    "print(\"Balanced accuracy score:\",balanced_accuracy_score(y_test,y_pred))\n",
    "print(\"roc_auc_score\",roc_auc_score(y_test, y_pred))\n",
    "fpr, tpr, unused = roc_curve(y_test,y_pred)\n",
    "print(\"auc\",auc(fpr,tpr))\n",
    "precision, recall, unused = precision_recall_curve(y_test, y_pred)\n",
    "print(\"prec, rec:\",precision,recall)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output of these metrics, the roc_auc_score, which measures the area under the roc curve, is .75. This is on a scale from 1 to 0, and 1 is perfect, so the score is ok. The recall is .94, which is ok for this scenario because it deals with preventative medicine. Only 6% of high risk patients will be misidentified. Based on the f1 score of .82, the precision and recall together are high, but not great for a high-pressure scenario like this medical context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "Now, you will build an analogous classifier, i.e., with the same objective as the one in the previous part, using a neural network structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "## build a MLP (multilayer perceptron) model to predict the outcome using \n",
    "# the same predictors as in the XGBoost model.\n",
    "# the MLP model should have at least 2 hidden layers, ReLU activation\n",
    "\n",
    "#TODO\n",
    "\n",
    "nn_model = nn.Sequential(\n",
    "    nn.Linear(13, 6),  # 1st hidden layer with ReLU activation\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 7),  # 2nd hidden layer with ReLU activation\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(7, 1)  # Output layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function and the optimizer\n",
    "loss = nn.BCELoss()\n",
    "learning_rate = 0.01  # You can adjust the learning rate based on your problem\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model on the training data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "train_tensor = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    nn_model.train()\n",
    "    totalLoss = 0\n",
    "\n",
    "    for vals, outcome in train_dataloader:\n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        output = torch.sigmoid(nn_model(vals))\n",
    "        loss_amt = loss(output, outcome.view(-1, 1))  # Reshape outcome to match output shape\n",
    "        totalLoss += loss_amt\n",
    "        \n",
    "        loss_amt.backward()\n",
    "        \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 48.38709677419355 %\n",
      "precision: 0.5555555555555556\n",
      "recall: 0.29411764705882354\n",
      "balanced accuracy: 0.631336405529954\n",
      "AUC: 0.5042016806722689\n"
     ]
    }
   ],
   "source": [
    "#compute the test set accuracy, as well as the metrics you picked to evaluate the xgboost model\n",
    "nn_model.eval()\n",
    "\n",
    "corr = 0\n",
    "tot = 0\n",
    "true_positive = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "true_negative = 0\n",
    "\n",
    "y = [] \n",
    "y_pred = []\n",
    "\n",
    "X_tensor_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_tensor_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "test_tensor = TensorDataset(X_tensor_test, y_tensor_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for vals, label in test_tensor:\n",
    "        outcome = nn_model(vals)\n",
    "\n",
    "        pred = (outcome >= .5).float().item()\n",
    "        predicted = pred == label.item()\n",
    "\n",
    "        tot += 1\n",
    "        y.append(label.item())\n",
    "        y_pred.append(pred)\n",
    "\n",
    "        if label.item() == 0:\n",
    "            if pred == 0:\n",
    "                true_negative += 1\n",
    "                corr += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "        else:\n",
    "            if pred == 0:\n",
    "                false_negative += 1\n",
    "            else:\n",
    "                corr += 1\n",
    "                true_positive += 1\n",
    "\n",
    "print(\"accuracy:\",(corr / tot)*100,\"%\")\n",
    "\n",
    "print(\"precision:\",true_positive / (true_positive + false_positive))\n",
    "print(\"recall:\",true_positive / (true_positive + false_negative))\n",
    "print(\"balanced accuracy:\",(recall + true_negative / (true_negative + false_positive)) / 2)\n",
    "print(\"AUC:\",roc_auc_score(y, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the NN-based classifier performs with respect to the XGBOOST one?**\n",
    "\n",
    "The XGBOOST classifier performs better than the NN. The XGBOOST model has a precision and recall of .72 and .94, which are ok in the context of this problem because while nearly 30% of healthy patients will be told that they have a problem, only 6% of at-risk patients are skipped. For the NN, the precision and recall both take a signficant hit; .55 and .29,respectively. This means that upwards of 70% of at-risk patients are told that they don't have a problem, which is much worse than a coin flip. The accuracy is 48.38%, which means that the classification overall is not helpful. Additionally, even though the metric isn't particularly meaningful for this problem due to the lbel distribution being even, the balanced accuracy score of .63 is not good. The AUC score of .5 also shows that this model is not effective at predicting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL QUESTION: Suppose that one of the classifiers you built performs very well, would you suggest using it to decide on the allocation of treatments to prevent heart attacks?**\n",
    "\n",
    "No. I would not because the recall is still not tuned highly enough. I would want an extremely high recall for a situation like this, where it is really important to not miss any at-risk patients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
